{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96217dfd",
   "metadata": {},
   "source": [
    "# Q-Learning in Reinforcement Learning \n",
    "\n",
    "Q-Learning is a Reinforcement learning policy that will find the next best action, given a current state. It chooses this action at random and aims to maximize the reward.\n",
    "\n",
    "Q-learning is a model-free, off-policy reinforcement learning that will find the best course of action, given the current state of the agent. Depending on where the agent is in the environment, it will decide the next action to be taken. \n",
    "\n",
    "Important Terms in Q-Learning\n",
    "\n",
    "1. States: The State, S, represents the current position of an agent in an environment. \n",
    "2. Action: The Action, A, is the step taken by the agent when it is in a particular state.\n",
    "3. Rewards: For every action, the agent will get a positive or negative reward.\n",
    "4. Episodes: When an agent ends up in a terminating state and canâ€™t take a new action.\n",
    "5. Q-Values: Used to determine how good an Action, A, taken at a particular state, S, is. Q (A, S).\n",
    "6. Temporal Difference: A formula used to find the Q-Value by using the value of current state and action and previous state and action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2926c4",
   "metadata": {},
   "source": [
    "In Q-Learning, the agent has an initial Q-Table with arbitary fixed values (could be zero). \n",
    "\n",
    "Initally, the agent will be in start position in the environment denoted as ($s_{t=0}$)\n",
    "\n",
    "\n",
    "At each time($t$), the agent \n",
    "- Performs an Action ($a_t$)\n",
    "- Observe the reward ($r_t$)\n",
    "- Enter to a new state ($s_{t+1}$)\n",
    "\n",
    "Then, based on operation the Q-Table updates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cca3fc",
   "metadata": {},
   "source": [
    "For updating the Q-table the algorithm uses the **Bellman Equation** which states\n",
    "\n",
    "$$Q(s_t, a_t) = R(s_t, a_t) + \\gamma * Max(Q(s_{t+1}, A)) $$\n",
    "\n",
    "where, \n",
    "- $s_t$ is current state\n",
    "- $a_t$ is current action \n",
    "- $R(s_t, a_t)$ is reward for current action and state\n",
    "- $\\gamma$ is discount factor\n",
    "- $s_{t+1}$ is the next state\n",
    "- $A$ is all actions\n",
    "- $Q(s_{t+1}, A)$ is the q-value for all action in next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26daafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from environment import Environment\n",
    "from generic_agent import GenericAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ede19",
   "metadata": {},
   "source": [
    "For understanding the Q-Learning technique, we are using Frozen Lake environment from OpenAI Gym. <br>\n",
    "To know more about [FrozenLake-v1](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)\n",
    "\n",
    "\n",
    "![Image](https://www.gymlibrary.dev/_images/frozen_lake.gif \"Frozen Lake Without Slippery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7d2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Environment Name\n",
    "env_name = 'FrozenLake-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac432a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Name:  FrozenLake\n",
      "Action Space Type:  DISCRETE\n",
      "Observation Space Type:  DISCRETE\n",
      "Observation Space:  Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "env = Environment(env_name, render_mode=\"rgb_array\", is_slippery=False, map_name=\"4x4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65729a39",
   "metadata": {},
   "source": [
    "## Q-Agent \n",
    "\n",
    "I am defining the Q-Agent, that will find the next best action based on current observations. \n",
    "\n",
    "### Algorithm \n",
    "\n",
    "1. Define the Q-Table with action and observation space. Initialize it to zeros.\n",
    "2. Select the best action for the observation from Q-Table\n",
    "    $$a = Max(Q(O_t, A)) $$\n",
    "    where, \n",
    "    - $O_t$ is the current observation\n",
    "    - $A$ is the all action\n",
    "    \n",
    "3. Update the Q-Table based on selection\n",
    "    $$Q(s_t, a_t) = R(s_t, a_t) + \\gamma * Max(Q(s_{t+1}, A)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4674ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent(GenericAgent):\n",
    "    \n",
    "    def __init__(self, env: gym.wrappers.time_limit.TimeLimit, learning_rate = 0.57, discount_rate = 0.97, epsilon=1):\n",
    "        \n",
    "        super(QAgent, self).__init__(env)\n",
    "        self.env = env\n",
    "        self.initalize_qtable()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.explore, self.exploit = 0, 0\n",
    "    \n",
    "    def initalize_qtable(self):\n",
    "        # Creating the Q Table with almost zero values\n",
    "        self.q_table = 1e-4 * np.random.random(size=[self.observation_size, self.action_size])\n",
    "#         self.q_table = np.zeros([self.observation_size, self.action_size])\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            self.explore += 1\n",
    "            return self.get_random_action(observation)\n",
    "        state_table = self.q_table[observation]\n",
    "        action = np.argmax(state_table) \n",
    "        self.exploit += 1\n",
    "        return action\n",
    "\n",
    "    def update_qtable(self, experience, action):\n",
    "        \n",
    "        observation, reward, terminate, truncate, info = experience\n",
    "        q_next = self.q_table[observation]\n",
    "        q_next = np.zeros([self.action_size]) if terminate else q_next\n",
    "        q_target = reward + self.discount_rate * np.max(q_next)\n",
    "        q_update = q_target - self.q_table[observation, action]\n",
    "        self.q_table[observation, action] += self.learning_rate * q_update\n",
    "        \n",
    "        \n",
    "        if terminate:\n",
    "            self.epsilon *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1ba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57eb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(env)\n",
    "\n",
    "# Training for 100 episodes\n",
    "prev_obs = env.observation_space.sample()\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a14153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd8f09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2940b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 999, Reward: 2.0\n",
      "Previous Observation: 1, Observation: 5\n",
      "Action: 1 Epsilon: 2.158562370532892e-05\n",
      "Exploration Step: 0 Exploitation Steps: 3\n",
      "\n",
      "\n",
      "[[9.45390334e-05 3.22905005e-05 9.74629211e-05 9.45390334e-05]\n",
      " [6.11335998e-05 6.33467581e-05 6.14463553e-05 6.14463553e-05]\n",
      " [8.20463740e-05 9.05931915e-05 8.78753957e-05 8.78753947e-05]\n",
      " [2.40144135e-05 4.25632189e-05 4.21758328e-05 2.93294315e-05]\n",
      " [6.39103060e-05 6.24836989e-05 6.44161843e-05 6.33987855e-05]\n",
      " [1.95174236e-18 0.00000000e+00 6.86415424e-16 1.55275973e-05]\n",
      " [7.91983556e-05 7.68224049e-05 7.39393840e-05 6.75280735e-05]\n",
      " [8.51983656e-05 4.42706545e-07 1.91936674e-06 7.96973046e-05]\n",
      " [9.23424475e-05 9.23451698e-05 7.15561013e-05 9.52012069e-05]\n",
      " [5.77591793e-05 4.64412436e-05 5.59538801e-05 5.30965776e-05]\n",
      " [5.21426284e-06 7.56921208e-05 6.45047619e-05 8.00714931e-05]\n",
      " [8.78075703e-05 1.72337399e-05 6.34275676e-06 6.72513690e-05]\n",
      " [9.97570651e-05 8.66987652e-07 3.02476199e-05 8.84291606e-05]\n",
      " [9.32850607e-05 8.88722556e-05 7.67467995e-05 6.24988048e-05]\n",
      " [7.19002292e-05 8.06379262e-05 7.93078007e-05 8.32676512e-05]\n",
      " [1.94756275e-05 3.77809711e-05 8.15114121e-01 5.68478233e-05]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(1000): \n",
    "    env.reset()\n",
    "    agent.explore = 0\n",
    "    agent.exploit = 0\n",
    "    terminate = False\n",
    "    sleep(0.1)\n",
    "    while not terminate:\n",
    "        env.render()\n",
    "        action = agent.get_action(prev_obs)\n",
    "        \n",
    "        (observe, reward, terminate, truncate, info) = env.step(action)\n",
    "        total_reward += reward\n",
    "        agent.update_qtable((observe, reward, terminate, truncate, info), action)\n",
    "        print(\"Episode: {}, Reward: {}\".format(episode, total_reward))\n",
    "        print(\"Previous Observation: {}, Observation: {}\".format(prev_obs, observe))\n",
    "        print(\"Action: {} Epsilon: {}\".format(action, agent.epsilon))\n",
    "        print(\"Exploration Step: {} Exploitation Steps: {}\\n\\n\".format(agent.explore, agent.exploit))\n",
    "        if terminate:\n",
    "            env.reset()\n",
    "        \n",
    "        prev_obs = observe\n",
    "        print(agent.q_table)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1075d0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.0, True, False, {'prob': 0.3333333333333333})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d9984ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680a847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
